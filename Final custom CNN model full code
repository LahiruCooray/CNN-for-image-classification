
import os
import numpy as np
from sklearn.model_selection import train_test_split
from tensorflow.keras.preprocessing.image import ImageDataGenerator
import pandas as pd
import matplotlib.pyplot as plt
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout, BatchNormalization
from tensorflow.keras.optimizers import Adam

# Download and unzip the dataset
!wget https://archive.ics.uci.edu/static/public/908/realwaste.zip -O /content/realwaste.zip
!unzip -q /content/realwaste.zip -d /content/dataset

# Define dataset path
dataset_path = '/content/dataset/realwaste-main/RealWaste'

# List all images and their categories
categories = os.listdir(dataset_path)
image_paths = []
labels = []

# Collect all image paths and corresponding labels
for label, category in enumerate(categories):
    category_path = os.path.join(dataset_path, category)
    for img_file in os.listdir(category_path):
        image_paths.append(os.path.join(category_path, img_file))
        labels.append(label)

# Convert to numpy arrays
image_paths = np.array(image_paths)
labels = np.array(labels)

# Define a function to preprocess all images
from tqdm import tqdm  # Install via pip if not already installed
def preprocess_all_images(image_paths, labels, target_size=(128, 128)):  # Adjusted size for faster processing
    datagen = ImageDataGenerator(rescale=1.0 / 255)
    processed_images = []
    for i, path in tqdm(enumerate(image_paths), total=len(image_paths), desc="Processing Images"):
        img = datagen.flow_from_dataframe(
            dataframe=pd.DataFrame({'filename': [path], 'class': [str(labels[i])]}),
            x_col='filename',
            y_col='class',
            target_size=target_size,
            batch_size=1,
            class_mode=None,
            shuffle=False
        )[0]
        processed_images.append(img[0])  # Extract the processed image array
    return np.array(processed_images), labels

# Preprocess all images
processed_images, processed_labels = preprocess_all_images(image_paths, labels)

# Split preprocessed data into train, validation, and test sets
train_images, temp_images, train_labels, temp_labels = train_test_split(
    processed_images, processed_labels, test_size=0.4, random_state=42
)
val_images, test_images, val_labels, test_labels = train_test_split(
    temp_images, temp_labels, test_size=0.5, random_state=42
)

# Print split results
print(f"Train samples: {len(train_images)}")
print(f"Validation samples: {len(val_images)}")
print(f"Test samples: {len(test_images)}")

# Calculate the number of classes based on unique labels in the training data
num_classes = len(np.unique(train_labels))  # Ensure train_labels is correctly passed here

# Define the CNN model
model = Sequential()

# First Convolutional Layer
model.add(Conv2D(32, (3, 3), activation='relu', input_shape=(128, 128, 3)))  # Updated input size
model.add(BatchNormalization())  # Add BatchNormalization here
model.add(MaxPooling2D(pool_size=(2, 2)))

# Second Convolutional Layer
model.add(Conv2D(64, (3, 3), activation='relu'))
model.add(BatchNormalization())  # Add BatchNormalization here
model.add(MaxPooling2D(pool_size=(2, 2)))

# Third Convolutional Layer
model.add(Conv2D(128, (3, 3), activation='relu'))
model.add(BatchNormalization())  # Add BatchNormalization here
model.add(MaxPooling2D(pool_size=(2, 2)))

# Fourth Convolutional Layer
model.add(Conv2D(256, (3, 3), activation='relu'))
model.add(BatchNormalization())  # Add BatchNormalization here
model.add(MaxPooling2D(pool_size=(2, 2)))

# Flatten the output
model.add(Flatten())

# Fully Connected Layer
model.add(Dense(128, activation='relu'))
model.add(Dropout(0.5))  # Increase dropout here as well

# Output Layer with softmax activation for multi-class classification
model.add(Dense(num_classes, activation='softmax'))

# Compile the model with Adam optimizer and sparse categorical crossentropy loss
optimizer = Adam(learning_rate=0.0001)
model.compile(optimizer=optimizer, loss='sparse_categorical_crossentropy', metrics=['accuracy'])

# Summary of the model
model.summary()


# Train the model
history = model.fit(
    x=train_images,
    y=train_labels,
    batch_size=32,
    epochs=20,
    validation_data=(val_images, val_labels)
)

# Plot training & validation loss and accuracy
plt.figure(figsize=(12, 6))

# Plot Training and Validation Loss
plt.subplot(1, 2, 1)
plt.plot(history.history['loss'], label='Training Loss')
plt.plot(history.history['val_loss'], label='Validation Loss')
plt.title('Training and Validation Loss')
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.legend()

# Plot Training and Validation Accuracy
plt.subplot(1, 2, 2)
plt.plot(history.history['accuracy'], label='Training Accuracy')
plt.plot(history.history['val_accuracy'], label='Validation Accuracy')
plt.title('Training and Validation Accuracy')
plt.xlabel('Epochs')
plt.ylabel('Accuracy')
plt.legend()

plt.show()

# Function to create the model
def create_model(learning_rate):
    model = Sequential()

    # First Convolutional Layer
    model.add(Conv2D(32, (3, 3), activation='relu', input_shape=(128, 128, 3)))  # Updated input size
    model.add(BatchNormalization())  # Add BatchNormalization here
    model.add(MaxPooling2D(pool_size=(2, 2)))

    # Second Convolutional Layer
    model.add(Conv2D(64, (3, 3), activation='relu'))
    model.add(BatchNormalization())  # Add BatchNormalization here
    model.add(MaxPooling2D(pool_size=(2, 2)))

    # Third Convolutional Layer
    model.add(Conv2D(128, (3, 3), activation='relu'))
    model.add(BatchNormalization())  # Add BatchNormalization here
    model.add(MaxPooling2D(pool_size=(2, 2)))

    # Fourth Convolutional Layer
    model.add(Conv2D(256, (3, 3), activation='relu'))
    model.add(BatchNormalization())  # Add BatchNormalization here
    model.add(MaxPooling2D(pool_size=(2, 2)))

    # Flatten the output
    model.add(Flatten())

    # Fully Connected Layer
    model.add(Dense(128, activation='relu'))
    model.add(Dropout(0.5))  # Increase dropout here as well

    # Output Layer with softmax activation for multi-class classification
    model.add(Dense(num_classes, activation='softmax'))

    # Compile the model with Adam optimizer and sparse categorical crossentropy loss
    optimizer = Adam(learning_rate=learning_rate)
    model.compile(optimizer=optimizer, loss='sparse_categorical_crossentropy', metrics=['accuracy'])

    return model

# List of different learning rates to experiment with
learning_rates = [0.0001, 0.001, 0.01, 0.1]

# Store histories of each model for plotting later
histories = []

# Train the model with different learning rates
for lr in learning_rates:
    print(f"Training model with learning rate: {lr}")
    model = create_model(lr)

    # Train the model for 20 epochs without early stopping
    history = model.fit(
        x=train_images,
        y=train_labels,
        batch_size=32,
        epochs=20,  # Train for a fixed 20 epochs
        validation_data=(val_images, val_labels),
        verbose=1
    )

    histories.append(history)

# Plot training & validation loss and accuracy for all models
plt.figure(figsize=(16, 8))

# Plot Training and Validation Loss
plt.subplot(1, 2, 1)
for i, lr in enumerate(learning_rates):
    plt.plot(histories[i].history['loss'], label=f'LR={lr} Training Loss')
    plt.plot(histories[i].history['val_loss'], label=f'LR={lr} Validation Loss')
plt.title('Training and Validation Loss for Different Learning Rates')
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.legend()

# Plot Training and Validation Accuracy
plt.subplot(1, 2, 2)
for i, lr in enumerate(learning_rates):
    plt.plot(histories[i].history['accuracy'], label=f'LR={lr} Training Accuracy')
    plt.plot(histories[i].history['val_accuracy'], label=f'LR={lr} Validation Accuracy')
plt.title('Training and Validation Accuracy for Different Learning Rates')
plt.xlabel('Epochs')
plt.ylabel('Accuracy')
plt.legend()

plt.show()


import matplotlib.pyplot as plt

# Plot training & validation loss with higher resolution and starting from epoch 2
plt.figure(figsize=(16, 8), dpi=200)  # Increase the DPI for higher resolution

# Plot Training and Validation Loss starting from epoch 2
for i, lr in enumerate(learning_rates):
    plt.plot(histories[i].history['loss'][1:], label=f'LR={lr} Training Loss')  # Skip the first epoch
    plt.plot(histories[i].history['val_loss'][1:], label=f'LR={lr} Validation Loss')  # Skip the first epoch

plt.title('Training and Validation Loss for Different Learning Rates (Starting from Epoch 2)')
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.legend()

# Save the figure with a higher DPI
plt.savefig('loss_graph_from_epoch_2_high_resolution.png', dpi=200)  # Save the plot with higher resolution
plt.show()


# Train the model with learning rate 0.0001
learning_rate = 0.0001
model = create_model(learning_rate)

# Train the model
history = model.fit(
    x=train_images,
    y=train_labels,
    batch_size=32,
    epochs=20,  # Train for a fixed 20 epochs
    validation_data=(val_images, val_labels),
    verbose=1
)

# Evaluate the model on the test set
test_loss, test_accuracy = model.evaluate(test_images, test_labels, verbose=0)
print(f"Model with learning rate {learning_rate}:")
print(f"Test Loss = {test_loss:.4f}")
print(f"Test Accuracy = {test_accuracy:.4f}")

# Predictions for the test set
predictions = model.predict(test_images)
predicted_classes = np.argmax(predictions, axis=1)

# Generate and plot confusion matrix
from sklearn.metrics import confusion_matrix, classification_report
import seaborn as sns
import matplotlib.pyplot as plt

# Confusion Matrix
cm = confusion_matrix(test_labels, predicted_classes)
plt.figure(figsize=(8, 6))
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')
plt.title('Confusion Matrix (LR=0.0001)')
plt.xlabel('Predicted')
plt.ylabel('True')
plt.show()

# Precision, Recall, and F1-score
from sklearn.metrics import precision_score, recall_score, f1_score

precision = precision_score(test_labels, predicted_classes, average='weighted')
recall = recall_score(test_labels, predicted_classes, average='weighted')
f1 = f1_score(test_labels, predicted_classes, average='weighted')

print(f"Precision: {precision:.4f}")
print(f"Recall: {recall:.4f}")
print(f"F1-Score: {f1:.4f}")

# Print classification report
print("\nClassification Report:")
print(classification_report(test_labels, predicted_classes))
