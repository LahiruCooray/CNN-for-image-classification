{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nipuni1313/CNN-for-image-classification/blob/main/Simple_convolutional_neural_network_to_perform_classification.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "giUO-25IvztX",
        "outputId": "9bafb586-67ef-4329-936a-be08caf9c321"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "SeNSNCgJWQuA"
      },
      "outputs": [],
      "source": [
        "# Create train, val, and test directories\n",
        "train_dir = '/content/drive/MyDrive/RealWasteDataset/train/'\n",
        "val_dir = '/content/drive/MyDrive/RealWasteDataset/validation/'\n",
        "test_dir = '/content/drive/MyDrive/RealWasteDataset/test/'\n",
        "import os\n",
        "import shutil\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Path to your image dataset\n",
        "dataset_path = '/content/drive/MyDrive/RealWaste/'\n",
        "\n",
        "# List all categories (assumes subfolders for each category)\n",
        "categories = os.listdir(dataset_path)\n",
        "\n",
        "\n",
        "\n",
        "for dir in [train_dir, val_dir, test_dir]:\n",
        "    if not os.path.exists(dir):\n",
        "        os.makedirs(dir)\n",
        "\n",
        "# Create directories for each category inside train, val, and test\n",
        "for category in categories:\n",
        "    os.makedirs(os.path.join(train_dir, category), exist_ok=True)\n",
        "    os.makedirs(os.path.join(val_dir, category), exist_ok=True)\n",
        "    os.makedirs(os.path.join(test_dir, category), exist_ok=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "BVL49DG_wj6G",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b18e4995-504b-4534-b73c-fd24af858802"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset split into train, validation, and test directories.\n"
          ]
        }
      ],
      "source": [
        "\n",
        "\n",
        "\n",
        "\n",
        "# Split the dataset into train, validation, and test sets\n",
        "for category in categories:\n",
        "    category_path = os.path.join('/content/drive/MyDrive/RealWaste/', category)\n",
        "\n",
        "    # Check if the category path exists and is a directory\n",
        "    if os.path.isdir(category_path) and len(os.listdir(category_path)) > 0:\n",
        "        images = os.listdir(category_path)\n",
        "\n",
        "        # Split the data into 60% train, 20% validation, 20% test\n",
        "        train_images, temp_images = train_test_split(images, test_size=0.4, random_state=42)\n",
        "        val_images, test_images = train_test_split(temp_images, test_size=0.5, random_state=42)\n",
        "\n",
        "        # Create target category directories in train, val, and test directories if they don't exist\n",
        "        os.makedirs(os.path.join(train_dir, category), exist_ok=True)\n",
        "        os.makedirs(os.path.join(val_dir, category), exist_ok=True)\n",
        "        os.makedirs(os.path.join(test_dir, category), exist_ok=True)\n",
        "\n",
        "        # Move files to respective directories\n",
        "        for image in train_images:\n",
        "            # Check if the image is already in the target directory\n",
        "            target_path = os.path.join(train_dir, category, image)\n",
        "            if not os.path.exists(target_path):  # Only move if the image doesn't exist\n",
        "                shutil.move(os.path.join(category_path, image), target_path)\n",
        "\n",
        "        for image in val_images:\n",
        "            # Check if the image is already in the target directory\n",
        "            target_path = os.path.join(val_dir, category, image)\n",
        "            if not os.path.exists(target_path):  # Only move if the image doesn't exist\n",
        "                shutil.move(os.path.join(category_path, image), target_path)\n",
        "\n",
        "        for image in test_images:\n",
        "            # Check if the image is already in the target directory\n",
        "            target_path = os.path.join(test_dir, category, image)\n",
        "            if not os.path.exists(target_path):  # Only move if the image doesn't exist\n",
        "                shutil.move(os.path.join(category_path, image), target_path)\n",
        "\n",
        "print(\"Dataset split into train, validation, and test directories.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "u95ufpYNyekD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9581ab60-606f-4a73-bc42-17fd1e1d82dd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 2914 images belonging to 9 classes.\n",
            "Found 950 images belonging to 9 classes.\n",
            "Found 958 images belonging to 9 classes.\n"
          ]
        }
      ],
      "source": [
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "# Initialize ImageDataGenerator for data augmentation (train only)\n",
        "train_datagen = ImageDataGenerator(\n",
        "    rescale=1./255,        # Normalize pixel values to [0, 1]\n",
        "    rotation_range=20,     # Random rotations\n",
        "    width_shift_range=0.2, # Random horizontal shift\n",
        "    height_shift_range=0.2, # Random vertical shift\n",
        "    shear_range=0.2,       # Shear transformation\n",
        "    zoom_range=0.2,        # Random zoom\n",
        "    horizontal_flip=True,  # Random horizontal flip\n",
        "    fill_mode='nearest'    # Fill empty pixels\n",
        ")\n",
        "\n",
        "# For validation and test, we just rescale the images\n",
        "val_test_datagen = ImageDataGenerator(rescale=1./255)\n",
        "\n",
        "# Load data using flow_from_directory\n",
        "train_data = train_datagen.flow_from_directory(\n",
        "    train_dir,  # Directory with training data\n",
        "    target_size=(128, 128),    # Resize images\n",
        "    batch_size=32,\n",
        "    class_mode='sparse'  # Set to 'sparse' to get integer labels\n",
        ")\n",
        "\n",
        "val_data = val_test_datagen.flow_from_directory(\n",
        "    val_dir,  # Directory with validation data\n",
        "    target_size=(128, 128),\n",
        "    batch_size=32,\n",
        "    class_mode='sparse'  # Set to 'sparse' to get integer labels\n",
        ")\n",
        "\n",
        "test_data = val_test_datagen.flow_from_directory(\n",
        "    test_dir,  # Directory with test data\n",
        "    target_size=(128, 128),\n",
        "    batch_size=32,\n",
        "    class_mode='sparse'  # Set to 'sparse' to get integer labels\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zmWHo0Gby9ad"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "# Define the CNN model\n",
        "model = Sequential()\n",
        "\n",
        "# First Convolutional Layer\n",
        "model.add(Conv2D(32, (3, 3), activation='relu', input_shape=(128, 128, 3)))\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "\n",
        "# Second Convolutional Layer\n",
        "model.add(Conv2D(64, (3, 3), activation='relu'))\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "\n",
        "# Flatten the output\n",
        "model.add(Flatten())\n",
        "\n",
        "# Fully Connected Layer\n",
        "model.add(Dense(128, activation='relu'))\n",
        "\n",
        "# Add Dropout to reduce overfitting\n",
        "model.add(Dropout(0.5))\n",
        "\n",
        "# Output Layer with softmax activation for multi-class classification\n",
        "model.add(Dense(train_data.num_classes, activation='softmax'))\n",
        "\n",
        "# Compile the model with Adam optimizer and sparse categorical crossentropy loss\n",
        "optimizer = Adam(learning_rate=0.0001)\n",
        "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "\n",
        "\n",
        "# Summary of the model\n",
        "model.summary()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dbV_CzgZzFI5"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "history = model.fit(\n",
        "    train_data,\n",
        "    steps_per_epoch=train_data.samples // train_data.batch_size,\n",
        "    epochs=20,\n",
        "    validation_data=val_data,\n",
        "    validation_steps=val_data.samples // val_data.batch_size\n",
        ")\n",
        "\n",
        "\n",
        "# Plot training & validation loss and accuracy\n",
        "plt.figure(figsize=(12, 6))\n",
        "\n",
        "# Plot Training and Validation Loss\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(history.history['loss'], label='Training Loss')\n",
        "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
        "plt.title('Training and Validation Loss')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "\n",
        "# Plot Training and Validation Accuracy\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(history.history['accuracy'], label='Training Accuracy')\n",
        "plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
        "plt.title('Training and Validation Accuracy')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend()\n",
        "\n",
        "plt.show()\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}